{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Семинар, учимся настраивать параметры бустинга\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем, например, xgboost:\n",
    "https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/python/python_api.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры можно разделить на группы. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Общие параметры алгоритма, например, тип бустинга\n",
    "\n",
    "2) Параметры обучения бустинга, например число итераций, learning rate\n",
    "\n",
    "3) Параметры построения деревьев, например, глубина, минимальное число элементов в листе\n",
    "\n",
    "4) Параметры стохастики, subsample, colsample\n",
    "\n",
    "5) Регуляриазция, например, lambda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПРАВИЛЬНОГО алгоритма для тюнинга нет, вот один из вариантов, которые часто используют на практике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Возьмите дефолтное параметры. Поставьте нужные loss и eval функции.  Измените максимальное  число деревьев, чтобы было не долго делать эксперименты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Подберите для него alpha, чтобы алгоритм не переобучался"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Подберите параметры сложности дерева, например,  глубину, мин число объектов в листах, мин значение для расщепления и тд"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Подберите паарметры стохастики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Подберите регуляризацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Уменьшите leraning rate и обучите максимальное число деревьев, чтобы не переобучалось"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте настроим параметры бустинга на нашем конкурсе!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026\n"
     ]
    }
   ],
   "source": [
    "doc_to_title = {}\n",
    "with open('docs_titles.tsv') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "print (len(doc_to_title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 15) (11690,) (11690,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, target_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_train.append(sorted(all_dist, reverse=True)[0:15]    )\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберите размер батча для обучения. Линейная модель не должна учиться дольше нескольких минут. \n",
    "\n",
    "Не забывайте использовать скейлер!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#clf = xgb.XGBClassifier(use_label_encoder = False, eval_metric = 'logloss',)\n",
    "#clf = lgb.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "train_scale = scaler.transform(X_train)\n",
    "dtrain = xgb.DMatrix(train_scale, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'verbosity' :'0', 'use_label_encoder' : False, 'n_estimators' : 800,\n",
    "    'objective' : 'binary:hinge' , 'reg_lambda' : 0.018, 'max_depth' : 6,\n",
    "    'gamma': 0.01, 'min_child_weight': 1, 'subsample' : 1, \n",
    "    'max_delta_step' : 1, 'base_score' : 4, 'learning_rate' : 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7832335329341318\n",
      "0.7839178785286568\n",
      "0.7828058169375532\n"
     ]
    }
   ],
   "source": [
    "# Вот тут делался перебор гиперпараметров, лучшие результаты вносились в params\n",
    "temp = [0.01, 0.1, 0.001]\n",
    "for i in temp:\n",
    "    clf = xgb.XGBClassifier(verbosity = 0, use_label_encoder = False, n_estimators = 500,\n",
    "                           objective = 'binary:hinge' , reg_lambda = 0.018, max_depth = 6,\n",
    "                           gamma = i, min_child_weight = 1, subsample = 0.8, \n",
    "                           max_delta_step = 1, base_score = 4, learning_rate = 0.1)\n",
    "    scores = cross_val_score(clf, train_scale, y_train, cv = 7)\n",
    "    score = scores.mean()\n",
    "    print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача: обучите на этих фичах (число фичей рассатриваем как гипераматр) бустинг, подберите параметры, засабмитьте на кагл. Побейте 0.62 паблик скор за сутки.  Удачи!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11690, 15), (11690,), (11690, 4), (16627, 3))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('test_groups.csv')\n",
    "X_train.shape, y_train.shape,train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "traingroups_titledata = {}\n",
    "for i in range(len(test_data)):\n",
    "    new_doc = test_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    #target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 15) (16627,)\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "groups_test = []\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title) in enumerate(docs):\n",
    "        groups_test.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_test.append(sorted(all_dist, reverse=True)[0:15]    )\n",
    "X_test = np.array(X_test)\n",
    "groups_test = np.array(groups_test)\n",
    "print (X_test.shape, groups_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=4, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0.01, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=1, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=500, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='binary:hinge', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=0.018, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', use_label_encoder=False,\n",
       "              validate_parameters=1, verbosity=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " clf = xgb.XGBClassifier(verbosity = 0, use_label_encoder = False, n_estimators = 500,\n",
    "                           objective = 'binary:hinge' , reg_lambda = 0.018, max_depth = 6,\n",
    "                           gamma = 0.01, min_child_weight = 1, subsample = 1, \n",
    "                           max_delta_step = 1, base_score = 4, learning_rate = 0.1)\n",
    "clf.fit(train_scale, y_train)\n",
    "#clf = xgb.train(params, dtrain, num_boost_round = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7781009409751926"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = scaler.transform(X_test)\n",
    "#dtest = xgb.DMatrix(X_scaled)\n",
    "pred = clf.predict(X_scaled)\n",
    "scores = cross_val_score(clf, train_scale, y_train, cv = 5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans1 = np.array(test_data['pair_id']).reshape(-1,1)\n",
    "ans = np.append(ans1, pred.reshape(-1,1), axis = 1)\n",
    "title = ['pair_id', 'target']\n",
    "df = pd.DataFrame(ans, columns = title)\n",
    "df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ник на kaggle: v nebe lunka (Илья Масс)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "402px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
